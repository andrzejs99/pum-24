{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad10fe87c955413f",
   "metadata": {},
   "source": [
    "# Lab 6: Visualizing the training process of neural networks. Hyperparameter tuning.\n",
    "\n",
    "In this lab, you will learn how to use [wandb](https://wandb.ai/) to visualize the training process of neural networks. We are going to build and train a feed-forward neural network for recognizing handwritten digits of the MNIST dataset. The training process will be visualized in the wandb dashboard, which will allow us to monitor the loss and accuracy of the model in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Feel free to create an account at [wandb.ai](https://wandb.ai/) before starting this lab.\n",
    "\n",
    "### A simple example of how to use wandb in a typical training loop is shown below:\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "wandb.login() # Log in to your wandb account\n",
    "\n",
    "# Start a new run\n",
    "\n",
    "some_config = {\n",
    "    'learning_rate': 0.01,\n",
    "    'layer_1_size': 128,\n",
    "    'layer_2_size': 64,\n",
    "    'batch_size': 32\n",
    "} # This is just an example of a configuration dictionary, you can put anything you want here\n",
    "\n",
    "wandb.init(project='mnist-classifier', config=some_config) # start a new run and log parameters\n",
    "\n",
    "# Here you would prepare your data, and initialize the model, optimizer, etc.\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    ...\n",
    "    wandb.log({'loss': loss, 'accuracy': accuracy})\n",
    "    # This will send the loss and accuracy to wandb and you can visualize it in the dashboard\n",
    "\n",
    "# End of the run\n",
    "wandb.finish()\n",
    "```\n",
    "\n",
    "The most important part is the `wandb.log()` function, which sends the data to the wandb dashboard. You can log any metric you want, not just loss and accuracy. The value passed to the function must be a dictionary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c701a2dc778ba9",
   "metadata": {},
   "source": [
    "## Exercise 1: Prepare data for training a mnist classifier (2 points)\n",
    "\n",
    "Before you start training a neural network, you need to prepare the data. In this exercise, you will prepare the MNIST dataset of handwritten digits for training a classifier. You should:\n",
    "\n",
    "1. Load the MNIST dataset using from `data/mnist_train.csv` and `data/mnist_test.csv` files.\n",
    "2. Normalize the data to the range [0, 1].\n",
    "3. Encode the labels using one-hot encoding.\n",
    "4. Create a PyTorch `Dataset` object for the training and test sets.\n",
    "5. Create a PyTorch `DataLoader` object for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73671250bc707ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('data/mnist-train.csv')\n",
    "test_data = pd.read_csv('data/mnist-test.csv')\n",
    "\n",
    "train_labels = pd.get_dummies(train_data['label'], columns=['label'])\n",
    "test_labels = pd.get_dummies(test_data['label'], columns=['label'])\n",
    "\n",
    "train_data = train_data.drop(columns='label') / 255\n",
    "test_data = test_data.drop(columns='label') / 255\n",
    "\n",
    "import torch\n",
    "\n",
    "train_data_tensor = torch.tensor(train_data.values, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.float32)\n",
    "test_data_tensor = torch.tensor(test_data.values, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5dc7e06a81f23f",
   "metadata": {},
   "source": [
    "## Exercise 2: Prepare the architecture of the neural network (2 points)\n",
    "\n",
    "In this exercise, you will prepare the architecture of the neural network. You should:\n",
    "\n",
    "1. Create a neural network class that inherits from `torch.nn.Module`.\n",
    "2. The neural network should have at least one hidden layer.\n",
    "3. Use ReLU activation functions after each but the output layer.\n",
    "4. Use a softmax activation function in the output layer to get the probabilities of each class.\n",
    "\n",
    "**Feel free to experiment with the architecture of your network** - try adding more hidden layers, changing the number of neurons in each layer, etc. You can also add a dropout layer or some other regularization technique and see if it improves the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5310cb52194df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,hidden_size=128):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.fc1=nn.Linear(784,hidden_size)\n",
    "        self.fc2=nn.Linear(hidden_size,10)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.softmax=nn.Softmax()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y=self.fc1(x)\n",
    "        x=self.relu(y)\n",
    "        output=self.fc2(x)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2efbd440294eb",
   "metadata": {},
   "source": [
    "## *Training PyTorch models on GPU\n",
    "\n",
    "**GPUs are optimized for performing matrix operations in parallel.** Although we call them \"graphics processing units\", they are actually very powerful processors that can be used for any kind of parallel computation, including training deep neural networks. In fact, data science is one of the most common applications of GPUs today, as can be seen by the revenue of companies like NVIDIA over the past few years. NVIDIA is a monopolist in the GPU market - in 2023, the company owned 92% of the data center GPU market share. As for 31 July, the 2024 revenue of NVIDIA was 60.92 billion USD, while the total revenue of 2020 was $10.92 billion. If someone benefits from the current deep learning hype, it is certainly NVIDIA.\n",
    "\n",
    "If you happen to have an NVIDIA GPU in your computer, you can use it to train your deep learning models, as PyTorch has excellent support for CUDA, which is NVIDIA's parallel computing API. To train a model on GPU, you need to explicitly tell PyTorch to move the model and the data to the GPU. \n",
    "\n",
    "Here is an example training loop that uses the GPU:\n",
    "\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # Check if a GPU is available\n",
    "\n",
    "# Initialize the model and move it to the GPU\n",
    "model = SomeNeuralNetwork().to(device)   # Move the model to the GPU\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch in data_loader:\n",
    "        X, y = batch\n",
    "        X, y = X.to(device), y.to(device)   # Move the tensors to GPU\n",
    "        \n",
    "        y_pred = model(X)   # Perform a forward pass (on the GPU)\n",
    "        loss = criterion(y_pred, y)   # Compute the loss (still on the GPU)\n",
    "        \n",
    "        ...  # The rest of the training loop\n",
    "        \n",
    "        y_pred = y_pred.detach().cpu()   # Move the predictions back to the CPU to do anything else with them\n",
    "```\n",
    "\n",
    "Note that **the model and all the tensors it uses for computation should be moved to the GPU**. You can do this by calling the `.to(device)` method on the model and the data tensors. If you want to move the data back to the CPU (to process it further, calculate metrics, visualize), you call the `.cpu()` method on the tensor.\n",
    "\n",
    "**Doing calculations on the GPU, you should be wary of few things:**\n",
    "\n",
    "* **The GPU has a limited amount of memory**, so you should be careful not to run out of memory. A typical graphics card has a few gigabytes of memory, so you should be fine with most models and datasets. However, moving very large tensors to the GPU can cause out-of-memory errors. That's one of the reasons why we use a dataloader and process the data in batches.\n",
    "* While the GPU is much faster than the CPU for large matrix operations, **transferring data between the CPU and the GPU is slow**. Therefore, it is best to minimize the number of data transfers between the CPU and the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94574b231d68216",
   "metadata": {},
   "source": [
    "## Exercise 3: Prepare the training loop (2 points)\n",
    "\n",
    "In this exercise, you will prepare the training loop. You should:\n",
    "\n",
    "1. Initialize the neural network.\n",
    "2. Define the loss function (CrossEntropyLoss) and the optimizer (Adam).\n",
    "3. Pass a dictionary with the configuration to wandb. This dictionary should contain all the hyperparameters of our model, including the learning rate, the size of the hidden layers, batch size, etc.\n",
    "4. Train the neural network. Each epoch should consist of a training and validation phase. You should log the loss and accuracy of the training and validation sets using wandb.\n",
    "5. Open you project at [wandb.ai](https://wandb.ai/) and see how cool it is!\n",
    "\n",
    "### Saving and loading the model\n",
    "As training can take some time, it is a good idea to save the model's state dictionary (its learned weights) to a file after training. You can do this with the following code:\n",
    "\n",
    "    torch.save(vae.state_dict(), 'vae.pth')\n",
    "    \n",
    "To load the model from the file, you can use the following code:\n",
    "\n",
    "    vae.load_state_dict(torch.load('vae.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d558333aa5fa0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "def train(model, train_loader, val_loader, epochs=100):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr to learing rate\n",
    "    loss = nn.MSELoss() \n",
    "    \n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    wandb.init(project='mnist-classifier')\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "       \n",
    "        model.train()   \n",
    "        train_loss = 0  \n",
    "        \n",
    "        for X_batch, y_batch in train_loader:   # load data batch-by-batch\n",
    "                #petla dla loadera             \n",
    "            optimizer.zero_grad()   \n",
    "         \n",
    "            y_pred = model(X_batch)  \n",
    "            \n",
    "            batch_loss = loss(y_pred, y_batch) \n",
    "       \n",
    "            batch_loss.backward()   #\n",
    "          \n",
    "            optimizer.step()    \n",
    "            \n",
    "            train_loss += batch_loss.item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_loss_history.append(train_loss)   \n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Train loss: {train_loss}')\n",
    "                  \n",
    "        model.eval()   \n",
    "        val_loss = 0   \n",
    "        \n",
    "        for X_batch, y_batch in val_loader:\n",
    "            \n",
    "            y_pred = model(X_batch)\n",
    "            val_loss += loss(y_pred, y_batch).item()  \n",
    "            \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_loss_history.append(val_loss)\n",
    "        print(f'Validation loss: {val_loss}')\n",
    "        wandb.log({'train_loss': train_loss,'val_loss': val_loss})\n",
    "\n",
    "    wandb.finish()\n",
    "    return model, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77990efcffab975",
   "metadata": {},
   "source": [
    "## Exercise 4: Easy hyperparameter tuning with wandb (2 points)\n",
    "\n",
    "Wandb allows you to perform hyperparameter tuning by automatically creating multiple runs with different hyperparameters and logging the performance of each run. Below is a brief instruction to `wandb` hyperparameter tuning, but you are more than welcome to find more information in the [official wandb guide](https://docs.wandb.ai/guides/sweeps/).\n",
    "\n",
    "Your task is to use wandb to perform hyperparameter tuning of the neural network, trying different values of the learning rate, batch size, and the size of the hidden layers. You can use the following hyperparameters:\n",
    "\n",
    "First, we need to define a dictionary with the hyperparameters that we want to tune. For example:\n",
    "\n",
    "```python\n",
    "parameters = {\n",
    "    'learning_rate': {'values': [0.01, 0.001, 0.0001]},\n",
    "    'batch_size': {'values': [32, 64, 128]},\n",
    "    'layer_1_size': {'values': [64, 128, 256]},\n",
    "    'layer_2_size': {'values': [32, 64, 128]}\n",
    "}\n",
    "```\n",
    "\n",
    "Then we need to create a dictionary with the configuration of the run:\n",
    "\n",
    "```python\n",
    "sweep_config = {\n",
    "    'name': 'mnist-sweep',\n",
    "    'method': 'grid',   # grid search, you can also try 'random' or 'bayes'\n",
    "    'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
    "    'parameters': parameters,   # that's the dictionary with the hyperparameters\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, we can use the `wandb.sweep` function to perform hyperparameter tuning:\n",
    "\n",
    "```python\n",
    "sweep_id = wandb.sweep(sweep_config, project='mnist-classifier')\n",
    "```\n",
    "\n",
    "After that, we can finally run the sweep:\n",
    "\n",
    "```python\n",
    "wandb.agent(sweep_id, function=train)\n",
    "```\n",
    "where `train` is a function that trains the model and logs the metrics to wandb. This function should take a `config` argument, which will contain the hyperparameters of the run. That is how wandb knows which hyperparameters to tune.\n",
    "\n",
    "1. Rewrite the VAE training loop into a function that takes a single dictionary `parameters` as an argument, initializes the model, optimizer, and criterion, and trains the model for a fixed number of epochs. The function should log the loss and accuracy of the training and validation sets to wandb.\n",
    "2. Create a dictionary with the hyperparameters that you want to tune.\n",
    "3. Create a sweep configuration dictionary.\n",
    "4. Run the sweep and monitor the results in the wandb dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f63fb35d55201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key='d8df1d7205f98685d35ca86003d2c2f1e06c4215')\n",
    "wandb.init(project='mnist_classifier')\n",
    "def train(model, train_loader, val_loader, epochs=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr to learing rate\n",
    "    loss = nn.MSELoss() #funkcja kosztu\n",
    "    \n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs): # epoch to przejscie przez caly dataset\n",
    "        # jak dataloader wyciagnie wszystko juz z datasetu to minie wtedy jedna epoka\n",
    "        model.train()   # set the model to training mode (some layers may behave differently in training and evaluation)\n",
    "        train_loss = 0  # this variable will accumulate the training loss\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:   # load data batch-by-batch\n",
    "                #petla dla loadera             \n",
    "            optimizer.zero_grad()   # clear the gradients\n",
    "            # trzeba wyzerowac gradienty na optimizerze\n",
    "            y_pred = model(X_batch)  # forward pass\n",
    "            \n",
    "            batch_loss = loss(y_pred, y_batch) # compute the loss\n",
    "            # liczymy blad (ustalony MSE)\n",
    "            batch_loss.backward()   # compute the gradients\n",
    "            # metoda backward odpala algorytm do liczenia gradientu (wiec nie musimy go liczyc recznie)\n",
    "            optimizer.step()    # update the weights\n",
    "            # \"hej, masz policzone gradienty, dobierz wagi tak, aby loss byl mniejszy nastepnym razem.\"\n",
    "            train_loss += batch_loss.item() # accumulate training loss\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader) # compute the average loss\n",
    "        train_loss_history.append(train_loss)   # save the loss for plotting\n",
    "        ###\n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Train loss: {train_loss}')\n",
    "                  \n",
    "        model.eval()   \n",
    "        val_loss = 0   \n",
    "        \n",
    "        for X_batch, y_batch in val_loader:\n",
    "            \n",
    "            y_pred = model(X_batch)\n",
    "            val_loss += loss(y_pred, y_batch).item()  \n",
    "            \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_loss_history.append(val_loss)\n",
    "        print(f'Validation loss: {val_loss}')\n",
    "        wandb.log({'train_loss': train_loss,'val_loss': val_loss})\n",
    "        #####\n",
    "\n",
    "    wandb.finish()\n",
    "    return model, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896fac35c516b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'learning_rate': {'values': [0.01, 0.001, 0.0001]},\n",
    "    'batch_size': {'values': [32, 64, 128]},\n",
    "    'layer_1_size': {'values': [64, 128, 256]},\n",
    "    'layer_2_size': {'values': [32, 64, 128]}\n",
    "}\n",
    "\n",
    "sweep_config = {\n",
    "    'name': 'mnist-sweep',\n",
    "    'method': 'bayes',\n",
    "    'metric': {'goal': 'maximize', 'name': 'accuracy'}, # if we want to maximize the accuracy\n",
    "    # remember to log the metric that you want to maximize or minimize!\n",
    "    'parameters': parameters,\n",
    "}\n",
    "model = NeuralNetwork()\n",
    "sweep_id = wandb.sweep(sweep_config, project='mnist-classifier')    # This will create a new sweep\n",
    "wandb.agent(sweep_id, lambda:train(model, train_loader, test_loader))   # This will start the hyperparameter tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9f68e-bc52-4c99-b4b4-fe23237038b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
